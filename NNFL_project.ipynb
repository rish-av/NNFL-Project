{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as io\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , Flatten, MaxPooling2D , Dropout\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import RandomUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model, Data Preprocessing and other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(inputshape,kernel_size1,kernel_size2,activationfunc,kernelinitializer,no_of_filters,pool_size,dense_layer_nodes1,num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(no_of_filters, kernel_size1, activation=activationfunc, input_shape=inputshape,kernel_initializer=kernelinitializer,bias_initializer='zeros'))\n",
    "    model.add(MaxPooling2D(pool_size = pool_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_layer_nodes1,activation=activationfunc,kernel_initializer=kernelinitializer))\n",
    "    model.add(Dense(num_classes,activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data,labels):\n",
    "    num_classes = np.max(labels)\n",
    "    data_in_classes = {x:[] for x in range(1,num_classes+1)}\n",
    "    (imgdm1,imgdm2,bands) = data.shape\n",
    "    for i in range(imgdm1):\n",
    "        for j in range(imgdm2):\n",
    "            l = labels[i][j]\n",
    "            if l!=0:\n",
    "                data_in_classes[l].append(np.array(data[i][j]))\n",
    "    #printfunc(data_in_classes)\n",
    "    y = []\n",
    "    X = []\n",
    "    for i in range(1,num_classes+1):\n",
    "        for j in range(len(data_in_classes[i])):\n",
    "            X.append(data_in_classes[i][j])\n",
    "            y.append(i)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = normalize(X,axis=1,return_norm=False)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> A separate Preprocessing Method for Indian Pines is required as we eliminate some classes which have very little sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_IndiaPines(data,labels):\n",
    "    num_classes = np.max(labels)\n",
    "    data_in_classes = {x:[] for x in range(1,num_classes+1)}\n",
    "    (imgdm1,imgdm2,bands) = data.shape\n",
    "    for i in range(imgdm1):\n",
    "        for j in range(imgdm2):\n",
    "            l = labels[i][j]\n",
    "            if l!=0:\n",
    "                data_in_classes[l].append(np.array(data[i][j]))\n",
    "    y = []\n",
    "    X = []\n",
    "    for i in range(1,17):\n",
    "        if(len(data_in_classes[i])>480):\n",
    "            for j in range(len(data_in_classes[i])):\n",
    "                X.append(data_in_classes[i][j])\n",
    "                y.append(i)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = normalize(X,axis=1,return_norm=False)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_labels(path,data_file_name):\n",
    "    return io.loadmat(path)[data_file_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y,test_size,random_state,no_bands):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state)\n",
    "    X_test = X_test.reshape(len(X_test),no_bands,1,1)\n",
    "    X_train = X_train.reshape(len(X_train),no_bands,1,1)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    y_train = y_train[:,1:]\n",
    "    y_test = y_test[:,1:]\n",
    "    return X_test,X_train,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,epoch,batchsize,lossused,X_test,X_train,y_test,y_train):\n",
    "    model.compile(optimizer=optimizer,loss=lossused,metrics = ['accuracy'])\n",
    "    filepath=\"weights.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = epoch,verbose=1,batch_size=batchsize,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Simulations on the Datasets of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data_labels('PaviaU.mat','paviaU')\n",
    "labels = load_data_labels('PaviaU_gt.mat','paviaU_gt')\n",
    "X,y = data_preprocess(dataset,labels)\n",
    "X_test,X_train,y_train,y_test = split_data(X,y,0.25,42,dataset.shape[2])\n",
    "inputshape = dataset[0][0].reshape(dataset[0][0].shape[0],1,1).shape\n",
    "kernelsize1 = np.array([11,1])\n",
    "kernelsize2 = np.array([3,1])\n",
    "initializer = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "filters = 20\n",
    "pool_size = np.array([3,1])\n",
    "nodes = 100\n",
    "classes = 9\n",
    "model = get_model(inputshape,kernelsize1,kernelsize2,'tanh',initializer,filters,pool_size,nodes,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 93, 1, 20)         240       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 1, 20)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 620)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               62100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 909       \n",
      "=================================================================\n",
      "Total params: 63,249\n",
      "Trainable params: 63,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28659 samples, validate on 14117 samples\n",
      "Epoch 1/20\n",
      "28659/28659 [==============================] - 8s 275us/step - loss: 0.7410 - acc: 0.7044 - val_loss: 0.6339 - val_acc: 0.7497\n",
      "Epoch 2/20\n",
      "28659/28659 [==============================] - 8s 269us/step - loss: 0.5563 - acc: 0.7862 - val_loss: 0.5075 - val_acc: 0.8134\n",
      "Epoch 3/20\n",
      "28659/28659 [==============================] - 8s 272us/step - loss: 0.4706 - acc: 0.8244 - val_loss: 0.4472 - val_acc: 0.8379\n",
      "Epoch 4/20\n",
      "28659/28659 [==============================] - 8s 272us/step - loss: 0.4276 - acc: 0.8384 - val_loss: 0.4756 - val_acc: 0.8240\n",
      "Epoch 5/20\n",
      "28659/28659 [==============================] - 8s 284us/step - loss: 0.4055 - acc: 0.8474 - val_loss: 0.3752 - val_acc: 0.8614\n",
      "Epoch 6/20\n",
      "28659/28659 [==============================] - 8s 273us/step - loss: 0.3918 - acc: 0.8513 - val_loss: 0.4122 - val_acc: 0.8464\n",
      "Epoch 7/20\n",
      "28659/28659 [==============================] - 8s 275us/step - loss: 0.3827 - acc: 0.8547 - val_loss: 0.3697 - val_acc: 0.8622\n",
      "Epoch 8/20\n",
      "28659/28659 [==============================] - 8s 273us/step - loss: 0.3727 - acc: 0.8586 - val_loss: 0.3476 - val_acc: 0.8729\n",
      "Epoch 9/20\n",
      "28659/28659 [==============================] - 8s 274us/step - loss: 0.3640 - acc: 0.8622 - val_loss: 0.3540 - val_acc: 0.8627\n",
      "Epoch 10/20\n",
      "28659/28659 [==============================] - 8s 278us/step - loss: 0.3518 - acc: 0.8691 - val_loss: 0.3311 - val_acc: 0.8753\n",
      "Epoch 11/20\n",
      "28659/28659 [==============================] - 8s 276us/step - loss: 0.3405 - acc: 0.8724 - val_loss: 0.4234 - val_acc: 0.8460\n",
      "Epoch 12/20\n",
      "28659/28659 [==============================] - 8s 276us/step - loss: 0.3300 - acc: 0.8765 - val_loss: 0.3098 - val_acc: 0.8855\n",
      "Epoch 13/20\n",
      "28659/28659 [==============================] - 8s 285us/step - loss: 0.3137 - acc: 0.8840 - val_loss: 0.2927 - val_acc: 0.8944\n",
      "Epoch 14/20\n",
      "28659/28659 [==============================] - 8s 277us/step - loss: 0.3026 - acc: 0.8874 - val_loss: 0.3026 - val_acc: 0.8846\n",
      "Epoch 15/20\n",
      "28659/28659 [==============================] - 8s 289us/step - loss: 0.2913 - acc: 0.8905 - val_loss: 0.2776 - val_acc: 0.8953\n",
      "Epoch 16/20\n",
      "28659/28659 [==============================] - 8s 280us/step - loss: 0.2825 - acc: 0.8944 - val_loss: 0.2770 - val_acc: 0.8956\n",
      "Epoch 17/20\n",
      "28659/28659 [==============================] - 8s 279us/step - loss: 0.2754 - acc: 0.8956 - val_loss: 0.2732 - val_acc: 0.8990\n",
      "Epoch 18/20\n",
      "28659/28659 [==============================] - 8s 280us/step - loss: 0.2682 - acc: 0.9000 - val_loss: 0.2560 - val_acc: 0.9055\n",
      "Epoch 19/20\n",
      "28659/28659 [==============================] - 8s 274us/step - loss: 0.2642 - acc: 0.9007 - val_loss: 0.2526 - val_acc: 0.9059\n",
      "Epoch 20/20\n",
      "28659/28659 [==============================] - 8s 272us/step - loss: 0.2569 - acc: 0.9032 - val_loss: 0.2597 - val_acc: 0.9019\n"
     ]
    }
   ],
   "source": [
    "train(model,'adam',20,10,'categorical_crossentropy',X_test,X_train,y_test,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = load_data_labels('Salinas_corrected.mat','salinas_corrected')\n",
    "labels1 = load_data_labels('Salinas_gt.mat','salinas_gt')\n",
    "X1,y1 = data_preprocess(dataset1,labels1)\n",
    "X_test1,X_train1,y_train1,y_test1 = split_data(X1,y1,0.25,42,dataset1.shape[2])\n",
    "inputshape1 = dataset1[0][0].reshape(dataset1[0][0].shape[0],1,1).shape\n",
    "kernelsize11 = np.array([11,1])\n",
    "kernelsize22 = np.array([3,1])\n",
    "initializer1 = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "filters = 20\n",
    "pool_size = np.array([3,1])\n",
    "nodes = 100\n",
    "classes1 = 16\n",
    "model1 = get_model(inputshape1,kernelsize11,kernelsize22,'tanh',initializer,filters,pool_size,nodes,classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 194, 1, 20)        240       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 1, 20)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               128100    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1616      \n",
      "=================================================================\n",
      "Total params: 129,956\n",
      "Trainable params: 129,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36266 samples, validate on 17863 samples\n",
      "Epoch 1/20\n",
      "36266/36266 [==============================] - 16s 453us/step - loss: 0.7845 - acc: 0.7067 - val_loss: 0.4445 - val_acc: 0.8332\n",
      "Epoch 2/20\n",
      "36266/36266 [==============================] - 15s 425us/step - loss: 0.4259 - acc: 0.8387 - val_loss: 0.4721 - val_acc: 0.8050\n",
      "Epoch 3/20\n",
      "36266/36266 [==============================] - 16s 441us/step - loss: 0.3875 - acc: 0.8500 - val_loss: 0.3880 - val_acc: 0.8434\n",
      "Epoch 4/20\n",
      "36266/36266 [==============================] - 16s 434us/step - loss: 0.3619 - acc: 0.8601 - val_loss: 0.3364 - val_acc: 0.8778\n",
      "Epoch 5/20\n",
      "36266/36266 [==============================] - 16s 440us/step - loss: 0.3444 - acc: 0.8648 - val_loss: 0.3256 - val_acc: 0.8680\n",
      "Epoch 6/20\n",
      "36266/36266 [==============================] - 16s 436us/step - loss: 0.3308 - acc: 0.8675 - val_loss: 0.3227 - val_acc: 0.8801\n",
      "Epoch 7/20\n",
      "36266/36266 [==============================] - 17s 460us/step - loss: 0.3218 - acc: 0.8709 - val_loss: 0.3112 - val_acc: 0.8847\n",
      "Epoch 8/20\n",
      "36266/36266 [==============================] - 17s 458us/step - loss: 0.3205 - acc: 0.8721 - val_loss: 0.3103 - val_acc: 0.8775\n",
      "Epoch 9/20\n",
      "36266/36266 [==============================] - 17s 463us/step - loss: 0.3074 - acc: 0.8764 - val_loss: 0.3834 - val_acc: 0.8241\n",
      "Epoch 10/20\n",
      "36266/36266 [==============================] - 17s 461us/step - loss: 0.3054 - acc: 0.8771 - val_loss: 0.3859 - val_acc: 0.8104\n",
      "Epoch 11/20\n",
      "36266/36266 [==============================] - 17s 456us/step - loss: 0.2989 - acc: 0.8796 - val_loss: 0.3005 - val_acc: 0.8750\n",
      "Epoch 12/20\n",
      "36266/36266 [==============================] - 17s 460us/step - loss: 0.2922 - acc: 0.8820 - val_loss: 0.2748 - val_acc: 0.8863\n",
      "Epoch 13/20\n",
      "36266/36266 [==============================] - 17s 463us/step - loss: 0.2895 - acc: 0.8820 - val_loss: 0.2788 - val_acc: 0.8894\n",
      "Epoch 14/20\n",
      "36266/36266 [==============================] - 17s 463us/step - loss: 0.2857 - acc: 0.8842 - val_loss: 0.2893 - val_acc: 0.8893\n",
      "Epoch 15/20\n",
      "36266/36266 [==============================] - 17s 464us/step - loss: 0.2834 - acc: 0.8823 - val_loss: 0.2944 - val_acc: 0.8716\n",
      "Epoch 16/20\n",
      "36266/36266 [==============================] - 17s 464us/step - loss: 0.2836 - acc: 0.8842 - val_loss: 0.2838 - val_acc: 0.8810\n",
      "Epoch 17/20\n",
      "36266/36266 [==============================] - 17s 461us/step - loss: 0.2831 - acc: 0.8843 - val_loss: 0.2708 - val_acc: 0.8861\n",
      "Epoch 18/20\n",
      "36266/36266 [==============================] - 17s 460us/step - loss: 0.2770 - acc: 0.8869 - val_loss: 0.2666 - val_acc: 0.8978\n",
      "Epoch 19/20\n",
      "36266/36266 [==============================] - 17s 466us/step - loss: 0.2792 - acc: 0.8868 - val_loss: 0.3344 - val_acc: 0.8610\n",
      "Epoch 20/20\n",
      "36266/36266 [==============================] - 17s 466us/step - loss: 0.2747 - acc: 0.8871 - val_loss: 0.2644 - val_acc: 0.8921\n"
     ]
    }
   ],
   "source": [
    "train(model1,'adam',20,10,'categorical_crossentropy',X_test1,X_train1,y_test1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = load_data_labels('Indian_pines.mat','indian_pines')\n",
    "labels2 = load_data_labels('Indian_pines_gt.mat','indian_pines_gt')\n",
    "X2,y2 = data_preprocess_IndiaPines(dataset2,labels2)\n",
    "X_test2,X_train2,y_train2,y_test2 = split_data(X2,y2,0.25,42,dataset2.shape[2])\n",
    "inputshape2 = dataset2[0][0].reshape(dataset2[0][0].shape[0],1,1).shape\n",
    "kernelsize3 = np.array([11,1])\n",
    "kernelsize4 = np.array([3,1])\n",
    "initializer11 = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "filters = 20\n",
    "pool_size = np.array([3,1])\n",
    "nodes = 100\n",
    "classes2 = np.max(y2)\n",
    "model2 = get_model(inputshape2,kernelsize3,kernelsize4,'tanh',initializer,filters,pool_size,nodes,classes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5866 samples, validate on 2890 samples\n",
      "Epoch 1/20\n",
      "5866/5866 [==============================] - 3s 492us/step - loss: 0.6507 - acc: 0.7532 - val_loss: 0.5555 - val_acc: 0.7969\n",
      "Epoch 2/20\n",
      "5866/5866 [==============================] - 3s 455us/step - loss: 0.6357 - acc: 0.7559 - val_loss: 0.6418 - val_acc: 0.7516\n",
      "Epoch 3/20\n",
      "5866/5866 [==============================] - 3s 462us/step - loss: 0.6301 - acc: 0.7545 - val_loss: 0.5861 - val_acc: 0.7754\n",
      "Epoch 4/20\n",
      "5866/5866 [==============================] - 3s 459us/step - loss: 0.6297 - acc: 0.7601 - val_loss: 0.5646 - val_acc: 0.7862\n",
      "Epoch 5/20\n",
      "5866/5866 [==============================] - 3s 466us/step - loss: 0.6215 - acc: 0.7615 - val_loss: 0.6910 - val_acc: 0.7270\n",
      "Epoch 6/20\n",
      "5866/5866 [==============================] - 3s 473us/step - loss: 0.6178 - acc: 0.7639 - val_loss: 0.5776 - val_acc: 0.7827\n",
      "Epoch 7/20\n",
      "5866/5866 [==============================] - 3s 480us/step - loss: 0.6155 - acc: 0.7629 - val_loss: 0.7736 - val_acc: 0.7000\n",
      "Epoch 8/20\n",
      "5866/5866 [==============================] - 3s 480us/step - loss: 0.6123 - acc: 0.7598 - val_loss: 0.6975 - val_acc: 0.7131\n",
      "Epoch 9/20\n",
      "5866/5866 [==============================] - 3s 512us/step - loss: 0.6164 - acc: 0.7596 - val_loss: 0.5922 - val_acc: 0.7661\n",
      "Epoch 10/20\n",
      "5866/5866 [==============================] - 3s 491us/step - loss: 0.6066 - acc: 0.7673 - val_loss: 0.5154 - val_acc: 0.8076\n",
      "Epoch 11/20\n",
      "5866/5866 [==============================] - 3s 480us/step - loss: 0.5964 - acc: 0.7658 - val_loss: 0.6515 - val_acc: 0.7460\n",
      "Epoch 12/20\n",
      "5866/5866 [==============================] - 3s 498us/step - loss: 0.6106 - acc: 0.7702 - val_loss: 0.6122 - val_acc: 0.7526\n",
      "Epoch 13/20\n",
      "5866/5866 [==============================] - 3s 480us/step - loss: 0.5767 - acc: 0.7804 - val_loss: 0.7448 - val_acc: 0.7152\n",
      "Epoch 14/20\n",
      "5866/5866 [==============================] - 3s 504us/step - loss: 0.5881 - acc: 0.7794 - val_loss: 0.5724 - val_acc: 0.7869\n",
      "Epoch 15/20\n",
      "5866/5866 [==============================] - 3s 506us/step - loss: 0.5812 - acc: 0.7826 - val_loss: 0.6630 - val_acc: 0.7329\n",
      "Epoch 16/20\n",
      "5866/5866 [==============================] - 3s 480us/step - loss: 0.5902 - acc: 0.7699 - val_loss: 0.6290 - val_acc: 0.7581\n",
      "Epoch 17/20\n",
      "5866/5866 [==============================] - 3s 496us/step - loss: 0.5892 - acc: 0.7748 - val_loss: 0.5247 - val_acc: 0.8042\n",
      "Epoch 18/20\n",
      "5866/5866 [==============================] - 3s 479us/step - loss: 0.5720 - acc: 0.7854 - val_loss: 0.6194 - val_acc: 0.7657\n",
      "Epoch 19/20\n",
      "5866/5866 [==============================] - 3s 475us/step - loss: 0.5553 - acc: 0.7878 - val_loss: 0.5567 - val_acc: 0.7858\n",
      "Epoch 20/20\n",
      "5866/5866 [==============================] - 3s 483us/step - loss: 0.5545 - acc: 0.7890 - val_loss: 0.5352 - val_acc: 0.7920\n"
     ]
    }
   ],
   "source": [
    "train(model2,'adam',20,10,'categorical_crossentropy',X_test2,X_train2,y_test2,y_train2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
